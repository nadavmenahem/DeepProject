{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "71ee7991d2c54e62a450af8e8c3b1c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6977e1598a5c4807b14ccf73d214c83d",
              "IPY_MODEL_0484e61a6ce7459981ede4bfcb883e0c",
              "IPY_MODEL_798486f15ce64eeeabdb5ce728c0d3ce"
            ],
            "layout": "IPY_MODEL_5ff78c26f1ba40f287a52427fcbb932f"
          }
        },
        "6977e1598a5c4807b14ccf73d214c83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c84310bdacf64134993818314770af50",
            "placeholder": "​",
            "style": "IPY_MODEL_70176f1e1e1f43c7985b750d78ad9cfc",
            "value": "probe: 100%"
          }
        },
        "0484e61a6ce7459981ede4bfcb883e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_362c33f186d14d818b4402f3426ee8a7",
            "max": 1744,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f94222fe6b647f5b9078534df29e5cd",
            "value": 1744
          }
        },
        "798486f15ce64eeeabdb5ce728c0d3ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e531302a5d440429e920fd099cd95a7",
            "placeholder": "​",
            "style": "IPY_MODEL_c09731f906bd4c8bbbd95f33b9897a40",
            "value": " 1744/1744 [03:48&lt;00:00,  8.07it/s]"
          }
        },
        "5ff78c26f1ba40f287a52427fcbb932f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c84310bdacf64134993818314770af50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70176f1e1e1f43c7985b750d78ad9cfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "362c33f186d14d818b4402f3426ee8a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f94222fe6b647f5b9078534df29e5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e531302a5d440429e920fd099cd95a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c09731f906bd4c8bbbd95f33b9897a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec6c9ce6d0614ab88a795419c7b9bb68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e16496017952475290022179532f143b",
              "IPY_MODEL_bf0c7af448f34d6089c34a7d2d4b9b46",
              "IPY_MODEL_57a5386ee8df430c9c9316d2f59be9e3"
            ],
            "layout": "IPY_MODEL_67b091637a5e43d99313df0c03581e02"
          }
        },
        "e16496017952475290022179532f143b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb79b6db653b45048b68f019541a546f",
            "placeholder": "​",
            "style": "IPY_MODEL_fd091359ea3a42fda33e92f6ef40724d",
            "value": "cache: 100%"
          }
        },
        "bf0c7af448f34d6089c34a7d2d4b9b46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237801d5492841b19edc3478f5e1f689",
            "max": 1744,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd0f75942136450bacaee94876a8a58d",
            "value": 1744
          }
        },
        "57a5386ee8df430c9c9316d2f59be9e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db30f185b5ed45fc81143d6b9be55d70",
            "placeholder": "​",
            "style": "IPY_MODEL_bb8155c272cf415bafe45f410609368b",
            "value": " 1744/1744 [04:59&lt;00:00,  7.03it/s]"
          }
        },
        "67b091637a5e43d99313df0c03581e02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb79b6db653b45048b68f019541a546f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd091359ea3a42fda33e92f6ef40724d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "237801d5492841b19edc3478f5e1f689": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd0f75942136450bacaee94876a8a58d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db30f185b5ed45fc81143d6b9be55d70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb8155c272cf415bafe45f410609368b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import soundfile as sf\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from google.colab import files\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "7BrcFhBcK_aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bh9TpqxzMQ34",
        "outputId": "51e54860-0b7e-4fbb-ac2e-40dbb8623ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "eSRCuNyZWByR",
        "outputId": "4276da63-78f4-4f88-fe7f-7c344caf2bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9f6ed57b-6020-4f09-90eb-4533442c3a6d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9f6ed57b-6020-4f09-90eb-4533442c3a6d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_utils.py to data_utils (1).py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_utils (1).py': b'# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80 data_utils.py \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\nfrom __future__ import annotations\\r\\n\\r\\nimport os\\r\\nimport torch\\r\\nimport torchaudio\\r\\nimport pandas as pd\\r\\nfrom sklearn.model_selection import train_test_split\\r\\nfrom torch.utils.data import Dataset, DataLoader\\r\\n\\r\\n__all__ = [\"DEAMHandler\", \"WaveDataset\"]\\r\\n\\r\\n\\r\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80  WaveDataset  \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\nclass WaveDataset(Dataset):\\r\\n    \"\"\"\\r\\n    Yields (wave_tensor, label_tensor):\\r\\n        wave  : torch.FloatTensor, shape = [1, T]\\r\\n        label : torch.FloatTensor, shape = [2]   (valence_mean, arousal_mean)\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, handler: \"DEAMHandler\", ids, target_sr=16_000, mono=True):\\r\\n        self.h, self.ids = handler, ids\\r\\n        self.target_sr, self.mono = target_sr, mono\\r\\n\\r\\n        # cache labels in a dict: sid -> (valence, arousal)\\r\\n        df = handler.static_annotations.set_index(\"song_id\")\\r\\n        self._labels = {\\r\\n            sid: (row.valence_mean, row.arousal_mean)\\r\\n            for sid, row in df.iterrows()\\r\\n        }\\r\\n\\r\\n    def __len__(self):  return len(self.ids)\\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        sid        = self.ids[idx]\\r\\n        wav, _     = self.h.get_waveform(sid,\\r\\n                                         target_sr=self.target_sr,\\r\\n                                         mono=self.mono)\\r\\n        val, aro   = self._labels[sid]\\r\\n        label      = torch.tensor([val, aro], dtype=torch.float32)\\r\\n        return wav, label\\r\\n\\r\\n\\r\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80  DEAMHandler  \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\nclass DEAMHandler:\\r\\n    \"\"\"\\r\\n    Lightweight wrapper around *your own* DEAM copy.\\r\\n\\r\\n    Parameters\\r\\n    ----------\\r\\n    audio_root : str\\r\\n        Folder that contains files named  <song_id>.wav   (or <song_id>.pt).\\r\\n    annotations_csv : str | None, default = None\\r\\n        Path to static_annotations.csv.  If None, we look for it:\\r\\n            \\xe2\\x80\\xa2 audio_root/static_annotations.csv\\r\\n            \\xe2\\x80\\xa2 parent_of_audio_root/static_annotations.csv\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, *, audio_root: str, annotations_csv: str | None = None):\\r\\n        # -------- verify / locate paths -----------------------------------\\r\\n        if not os.path.isdir(audio_root):\\r\\n            raise FileNotFoundError(f\"audio_root not found: {audio_root!r}\")\\r\\n        self.audio_dir = os.path.join(audio_root, \"songs\")\\r\\n        if not os.path.isdir(self.audio_dir):\\r\\n            raise FileNotFoundError(f\"songs/ directory not found in {audio_root}\")\\r\\n\\r\\n        if annotations_csv is None:\\r\\n            candidates = [\\r\\n                os.path.join(audio_root, \"static_annotations.csv\"),\\r\\n                os.path.join(os.path.dirname(audio_root), \"static_annotations.csv\"),\\r\\n            ]\\r\\n            annotations_csv = next((p for p in candidates if os.path.isfile(p)), None)\\r\\n        if annotations_csv is None or not os.path.isfile(annotations_csv):\\r\\n            raise FileNotFoundError(\\r\\n                \"static_annotations.csv not found.\\\\n\"\\r\\n                \"Place it inside your audio folder or pass annotations_csv=\\'\\xe2\\x80\\xa6\\'\"\\r\\n            )\\r\\n\\r\\n        # -------- load annotations ---------------------------------------\\r\\n        self.static_annotations = pd.read_csv(annotations_csv)\\r\\n        self.static_annotations.columns = self.static_annotations.columns.str.strip()  # NEW\\r\\n        # remove stray leading/trailing spaces sometimes found in the CSV header\\r\\n        self.static_annotations.columns = (\\r\\n            self.static_annotations.columns.str.strip()\\r\\n        )\\r\\n        print(\"\\xe2\\x9c\\x94 static annotations :\", annotations_csv)\\r\\n        print(\"\\xe2\\x9c\\x94 audio root         :\", self.audio_dir)\\r\\n\\r\\n    # ------------------------------------------------------------------ #\\r\\n    def get_waveform(\\r\\n        self,\\r\\n        song_id: int,\\r\\n        *,\\r\\n        target_sr: int | None = 16_000,\\r\\n        mono: bool = True,\\r\\n        verbose: bool = False,\\r\\n    ):\\r\\n        \"\"\"\\r\\n        Return (wave_tensor, sr).  Search order in audio_root:\\r\\n\\r\\n            1. <song_id>.pt   (pre-saved torch tensor)    \\xe2\\x80\\x93 fastest\\r\\n            2. <song_id>.wav  (PCM, no decode penalty)\\r\\n\\r\\n        Raises FileNotFoundError if neither exists.\\r\\n        \"\"\"\\r\\n        for ext in (\".pt\", \".wav\"):\\r\\n            path = os.path.join(self.audio_dir, f\"{song_id:04d}{ext}\")\\r\\n            if os.path.isfile(path):\\r\\n                break\\r\\n        else:\\r\\n            raise FileNotFoundError(f\"no .pt or .wav for song_id {song_id:04d}\")\\r\\n\\r\\n        # ----- cached tensor --------------------------------------------\\r\\n        if ext == \".pt\":\\r\\n            wav = torch.load(path, map_location=\"cpu\").float()\\r\\n            # legacy caches may be 1-D \\xe2\\x86\\x92 add channel dim\\r\\n            if wav.dim() == 1:\\r\\n                print(f\"Warning: {path} is 1-D, adding channel dim - should not happen\")\\r\\n                wav = wav.unsqueeze(0)\\r\\n            sr  = target_sr or 16_000\\r\\n            if verbose:\\r\\n                print(f\" loaded tensor {path}  {tuple(wav.shape)}\")\\r\\n            return wav, sr\\r\\n\\r\\n        # ----- WAV via torchaudio ---------------------------------------\\r\\n        wav, sr = torchaudio.load(path)          # [ch, T]\\r\\n        if mono and wav.shape[0] > 1:\\r\\n            wav = wav.mean(dim=0, keepdim=True)\\r\\n        if target_sr and sr != target_sr:\\r\\n            wav = torchaudio.functional.resample(wav, sr, target_sr)\\r\\n            sr  = target_sr\\r\\n        if verbose:\\r\\n            print(f\" decoded {path}  {tuple(wav.shape)} @ {sr} Hz\")\\r\\n        return wav, sr\\r\\n\\r\\n    # ------------------------------------------------------------------ #\\r\\n    def build_dataloaders(\\r\\n        self,\\r\\n        *,\\r\\n        batch_size: int = 8,\\r\\n        val_split: float = 0.15,\\r\\n        test_split: float = 0.15,\\r\\n        target_sr: int = 16_000,\\r\\n        mono: bool = True,\\r\\n        shuffle: bool = True,\\r\\n        num_workers: int = 2,\\r\\n    ):\\r\\n        \"\"\"\\r\\n        Convenience splitter.  Returns:\\r\\n\\r\\n            train_loader, val_loader, test_loader\\r\\n        \"\"\"\\r\\n        ids = self.static_annotations[\"song_id\"].tolist()\\r\\n\\r\\n        # stratify by binary valence (\\xe2\\x89\\xa55) so splits look similar\\r\\n        labels = (self.static_annotations[\"valence_mean\"] >= 5).astype(int).tolist()\\r\\n\\r\\n        train_ids, tmp_ids, train_lbl, tmp_lbl = train_test_split(\\r\\n            ids, labels, test_size=val_split + test_split,\\r\\n            stratify=labels, random_state=42\\r\\n        )\\r\\n        rel_val = val_split / (val_split + test_split)\\r\\n        val_ids, test_ids = train_test_split(\\r\\n            tmp_ids, test_size=1 - rel_val,\\r\\n            stratify=tmp_lbl, random_state=42\\r\\n        )\\r\\n\\r\\n        def _mk(ids_, shuf):\\r\\n            ds = WaveDataset(self, ids_, target_sr=target_sr, mono=mono)\\r\\n            return DataLoader(ds, batch_size=batch_size, shuffle=shuf,\\r\\n                              num_workers=num_workers, pin_memory=True,\\r\\n                              drop_last=True)\\r\\n\\r\\n        return _mk(train_ids, shuffle), _mk(val_ids, False), _mk(test_ids, False)\\r\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from data_utils_old import DEAMHandler\n",
        "\n",
        "handler = DEAMHandler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_s4o7HzBL_-o",
        "outputId": "ecb10265-f5f6-4bc9-bbdf-55155669c74d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading DEAM via kagglehub …\n",
            "Dataset root: /kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music\n",
            "Audio directory: /kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_audio/MEMD_audio\n",
            "\n",
            "Dataset Preview:\n",
            "   song_id  valence_mean  valence_std  arousal_mean  arousal_std\n",
            "0        2           3.1         0.94           3.0         0.63\n",
            "1        3           3.5         1.75           3.3         1.62\n",
            "2        4           5.7         1.42           5.5         1.63\n",
            "3        5           4.4         2.01           5.3         1.85\n",
            "4        7           5.8         1.47           6.4         1.69\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1744 entries, 0 to 1743\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   song_id       1744 non-null   int64  \n",
            " 1   valence_mean  1744 non-null   float64\n",
            " 2   valence_std   1744 non-null   float64\n",
            " 3   arousal_mean  1744 non-null   float64\n",
            " 4   arousal_std   1744 non-null   float64\n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 68.3 KB\n",
            "None\n",
            "\n",
            "Summary Statistics:\n",
            "           song_id  valence_mean  valence_std  arousal_mean  arousal_std\n",
            "count  1744.000000   1744.000000  1744.000000   1744.000000  1744.000000\n",
            "mean   1062.077982      4.903492     1.521950      4.812603     1.466778\n",
            "std     584.299005      1.173920     0.385496      1.289368     0.367197\n",
            "min       2.000000      1.600000     0.300000      1.600000     0.460000\n",
            "25%     536.750000      4.100000     1.270000      3.800000     1.200000\n",
            "50%    1128.500000      4.900000     1.510000      4.900000     1.480000\n",
            "75%    1564.250000      5.800000     1.780000      5.800000     1.720000\n",
            "max    2000.000000      8.400000     2.900000      8.100000     2.590000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wave10, sr = handler.get_waveform(10, verbose=True) # DEAM song #10\n",
        "print(wave10.shape, sr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tLVO0wCTP7p",
        "outputId": "2b114dd1-6882-4bc1-ef78-c3a1da784dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio loaded: 720976 samples at 16000 Hz\n",
            "(720976,) 16000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CACHE_DIR = \"wav_cache\"\n",
        "TARGET_SR = 16_000                  # <— pick whatever you like (16 k, 22 050 …)\n",
        "\n",
        "present_ids  = sorted(handler.static_annotations[\"song_id\"].tolist())\n",
        "\n",
        "# ─── Probe once to find longest duration at the *chosen* SR ────────────────\n",
        "lengths = []\n",
        "\n",
        "print(f\"🔎 Scanning {len(present_ids)} songs at {TARGET_SR} Hz …\")\n",
        "for sid in present_ids:\n",
        "    wav, _ = handler.get_waveform(sid, target_sr=TARGET_SR, mono=True)\n",
        "    lengths.append(wav.shape[-1])\n",
        "\n",
        "MAX_LEN  = max(lengths)\n",
        "MAX_SECS = MAX_LEN / TARGET_SR\n",
        "\n",
        "print(f\"✔ Common sample-rate : {TARGET_SR} Hz\")\n",
        "print(f\"✔ Longest clip       : {MAX_LEN} samples ≈ {MAX_SECS:.2f} s\\n\")\n",
        "\n",
        "# ─── Caching all IDs (present or missing) ──────────────────────────────────\n",
        "os.makedirs(CACHE_DIR, exist_ok=True)\n",
        "\n",
        "MAX_ID     = max(present_ids)\n",
        "ID_WIDTH   = len(str(MAX_ID))\n",
        "\n",
        "print(f\"💾 Writing {MAX_ID} fixed-length tensors into {CACHE_DIR}/ …\\n\")\n",
        "\n",
        "for idx, sid in enumerate(range(1, MAX_ID + 1), start=1):\n",
        "    sid_str   = str(sid).zfill(ID_WIDTH)\n",
        "    out_path  = os.path.join(CACHE_DIR, f\"{sid_str}.pt\")\n",
        "    remaining = MAX_ID - idx\n",
        "\n",
        "    if os.path.exists(out_path):\n",
        "        print(f\"[{idx}/{MAX_ID}] {sid_str}  ✔ already cached  (remaining: {remaining})\")\n",
        "        continue\n",
        "\n",
        "    if sid in present_ids:\n",
        "        wav, _ = handler.get_waveform(sid, target_sr=TARGET_SR, mono=True)\n",
        "        wav = wav.squeeze(0)[:MAX_LEN]                    # 1-D\n",
        "        if wav.numel() < MAX_LEN:\n",
        "            wav = F.pad(wav, (0, MAX_LEN - wav.numel()))\n",
        "        status = \"✅ cached\"\n",
        "    else:\n",
        "        wav = torch.zeros(MAX_LEN)\n",
        "        status = \"Ⓜ missing → silence\"\n",
        "\n",
        "    torch.save(wav.unsqueeze(0), out_path)                # [1, MAX_LEN]\n",
        "    print(f\"[{idx}/{MAX_ID}] {sid_str}  {status}  (remaining: {remaining})\")\n",
        "\n",
        "print(\"\\n🎉 Done! All IDs now have uniform-SR tensors in wav_cache/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "iS4awcwBLHKi",
        "outputId": "961b9e9c-aaeb-4311-870e-4b6972cde157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Scanning 1744 songs at 16000 Hz …\n",
            "✔ Common sample-rate : 16000 Hz\n",
            "✔ Longest clip       : 726579 samples ≈ 45.41 s\n",
            "\n",
            "💾 Writing 2000 fixed-length tensors into wav_cache/ …\n",
            "\n",
            "[1/2000] 0001  Ⓜ missing → silence  (remaining: 1999)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot select an axis to squeeze out which has size not equal to one",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-1642257528>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpresent_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_waveform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_SR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmono\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m]\u001b[0m                    \u001b[0;31m# 1-D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mwav\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mwav\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot select an axis to squeeze out which has size not equal to one"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv proc_cache/*.pt /content/drive/MyDrive/Colab\\ Notebooks/DeepProject/DEAM_wav/"
      ],
      "metadata": {
        "id": "Anv8T2G-PVvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUDIO_DIR = \"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav\"\n",
        "\n",
        "# 3) Output file\n",
        "csv_out = f\"{AUDIO_DIR}/static_annotations.csv\"\n",
        "\n",
        "# 4) Save the DataFrame that’s already loaded in memory\n",
        "#    (replace `old_handler` with whatever variable holds your first DEAMHandler)\n",
        "handler.static_annotations.to_csv(csv_out, index=False)\n",
        "\n",
        "print(f\"✔ Saved static annotations to:\\n   {csv_out}\")"
      ],
      "metadata": {
        "id": "T12RbzQ5bgtx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d2b5e1-bad0-4dce-b51e-d345bcc4e9f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Saved static annotations to:\n",
            "   /content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav/static_annotations.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "ROOT  = Path(\"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav\")\n",
        "SONGS = ROOT / \"songs\"\n",
        "SONGS.mkdir(exist_ok=True)\n",
        "\n",
        "# 2️⃣  Move any loose .pt files into DEAM_wav/songs/\n",
        "for pt in ROOT.glob(\"*.pt\"):\n",
        "    target = SONGS / pt.name\n",
        "    if not target.exists():               # don’t overwrite\n",
        "        shutil.move(pt, target)\n",
        "        print(f\"→ moved {pt.name} to songs/\")\n",
        "\n",
        "# 3️⃣  Save the static annotations beside songs/\n",
        "csv_out = ROOT / \"annotations.csv\"\n",
        "handler.static_annotations.to_csv(csv_out, index=False)\n",
        "print(f\"\\n✔ annotations.csv saved to {csv_out}\")\n",
        "\n",
        "print(\"\\nFolder structure now:\")\n",
        "!tree -L 2 \"{ROOT}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIBVvMKd7mKj",
        "outputId": "9a62ffa2-159e-4c1b-88b4-756ede379f18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✔ annotations.csv saved to /content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav/annotations.csv\n",
            "\n",
            "Folder structure now:\n",
            "/bin/bash: line 1: tree: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QBesuKGVB5A0",
        "outputId": "d16c3ee3-c93d-4548-f14a-42eabbdfeb5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-de5dc1eb-94c8-48de-bd27-bc89d5246eb6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-de5dc1eb-94c8-48de-bd27-bc89d5246eb6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_utils.py to data_utils.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_utils.py': b'# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80 data_utils.py \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\nfrom __future__ import annotations\\r\\n\\r\\nimport os\\r\\nimport torch\\r\\nimport torchaudio\\r\\nimport pandas as pd\\r\\nfrom sklearn.model_selection import train_test_split\\r\\nfrom torch.utils.data import Dataset, DataLoader\\r\\n\\r\\n__all__ = [\"DEAMHandler\", \"WaveDataset\"]\\r\\n\\r\\n\\r\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80  WaveDataset  \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\nclass WaveDataset(Dataset):\\r\\n    \"\"\"\\r\\n    Yields (wave_tensor, label_tensor):\\r\\n        wave  : torch.FloatTensor, shape = [1, T]\\r\\n        label : torch.FloatTensor, shape = [2]   (valence_mean, arousal_mean)\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, handler: \"DEAMHandler\", ids, target_sr=16_000, mono=True):\\r\\n        self.h, self.ids = handler, ids\\r\\n        self.target_sr, self.mono = target_sr, mono\\r\\n\\r\\n        # cache labels in a dict: sid -> (valence, arousal)\\r\\n        df = handler.static_annotations.set_index(\"song_id\")\\r\\n        self._labels = {\\r\\n            sid: (row.valence_mean, row.arousal_mean)\\r\\n            for sid, row in df.iterrows()\\r\\n        }\\r\\n\\r\\n    def __len__(self):  return len(self.ids)\\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        sid        = self.ids[idx]\\r\\n        wav, _     = self.h.get_waveform(sid,\\r\\n                                         target_sr=self.target_sr,\\r\\n                                         mono=self.mono)\\r\\n        val, aro   = self._labels[sid]\\r\\n        label      = torch.tensor([val, aro], dtype=torch.float32)\\r\\n        return wav, label\\r\\n\\r\\n\\r\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80  DEAMHandler  \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\nclass DEAMHandler:\\r\\n    \"\"\"\\r\\n    Lightweight wrapper around *your own* DEAM copy.\\r\\n\\r\\n    Parameters\\r\\n    ----------\\r\\n    audio_root : str\\r\\n        Folder that contains files named  <song_id>.wav   (or <song_id>.pt).\\r\\n    annotations_csv : str | None, default = None\\r\\n        Path to static_annotations.csv.  If None, we look for it:\\r\\n            \\xe2\\x80\\xa2 audio_root/static_annotations.csv\\r\\n            \\xe2\\x80\\xa2 parent_of_audio_root/static_annotations.csv\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, *, audio_root: str, annotations_csv: str | None = None):\\r\\n        # -------- verify / locate paths -----------------------------------\\r\\n        if not os.path.isdir(audio_root):\\r\\n            raise FileNotFoundError(f\"audio_root not found: {audio_root!r}\")\\r\\n        self.audio_dir = os.path.join(audio_root, \"songs\")\\r\\n        if not os.path.isdir(self.audio_dir):\\r\\n            raise FileNotFoundError(f\"songs/ directory not found in {audio_root}\")\\r\\n\\r\\n        if annotations_csv is None:\\r\\n            candidates = [\\r\\n                os.path.join(audio_root, \"static_annotations.csv\"),\\r\\n                os.path.join(os.path.dirname(audio_root), \"static_annotations.csv\"),\\r\\n            ]\\r\\n            annotations_csv = next((p for p in candidates if os.path.isfile(p)), None)\\r\\n        if annotations_csv is None or not os.path.isfile(annotations_csv):\\r\\n            raise FileNotFoundError(\\r\\n                \"static_annotations.csv not found.\\\\n\"\\r\\n                \"Place it inside your audio folder or pass annotations_csv=\\'\\xe2\\x80\\xa6\\'\"\\r\\n            )\\r\\n\\r\\n        # -------- load annotations ---------------------------------------\\r\\n        self.static_annotations = pd.read_csv(annotations_csv)\\r\\n        self.static_annotations.columns = self.static_annotations.columns.str.strip()  # NEW\\r\\n        # remove stray leading/trailing spaces sometimes found in the CSV header\\r\\n        self.static_annotations.columns = (\\r\\n            self.static_annotations.columns.str.strip()\\r\\n        )\\r\\n        print(\"\\xe2\\x9c\\x94 static annotations :\", annotations_csv)\\r\\n        print(\"\\xe2\\x9c\\x94 audio root         :\", self.audio_dir)\\r\\n\\r\\n    # ------------------------------------------------------------------ #\\r\\n    def get_waveform(\\r\\n        self,\\r\\n        song_id: int,\\r\\n        *,\\r\\n        target_sr: int | None = 16_000,\\r\\n        mono: bool = True,\\r\\n        verbose: bool = False,\\r\\n    ):\\r\\n        \"\"\"\\r\\n        Return (wave_tensor, sr).  Search order in audio_root:\\r\\n\\r\\n            1. <song_id>.pt   (pre-saved torch tensor)    \\xe2\\x80\\x93 fastest\\r\\n            2. <song_id>.wav  (PCM, no decode penalty)\\r\\n\\r\\n        Raises FileNotFoundError if neither exists.\\r\\n        \"\"\"\\r\\n        for ext in (\".pt\", \".wav\"):\\r\\n            path = os.path.join(self.audio_dir, f\"{song_id}{ext}\")\\r\\n            if os.path.isfile(path):\\r\\n                break\\r\\n        else:\\r\\n            raise FileNotFoundError(f\"no .pt or .wav for song_id {song_id}\")\\r\\n\\r\\n        # ----- cached tensor --------------------------------------------\\r\\n        if ext == \".pt\":\\r\\n            wav = torch.load(path, map_location=\"cpu\").float()\\r\\n            # legacy caches may be 1-D \\xe2\\x86\\x92 add channel dim\\r\\n            if wav.dim() == 1:\\r\\n                print(f\"Warning: {path} is 1-D, adding channel dim - should not happen\")\\r\\n                wav = wav.unsqueeze(0)\\r\\n            sr  = target_sr or 16_000\\r\\n            if verbose:\\r\\n                print(f\" loaded tensor {path}  {tuple(wav.shape)}\")\\r\\n            return wav, sr\\r\\n\\r\\n        # ----- WAV via torchaudio ---------------------------------------\\r\\n        wav, sr = torchaudio.load(path)          # [ch, T]\\r\\n        if mono and wav.shape[0] > 1:\\r\\n            wav = wav.mean(dim=0, keepdim=True)\\r\\n        if target_sr and sr != target_sr:\\r\\n            wav = torchaudio.functional.resample(wav, sr, target_sr)\\r\\n            sr  = target_sr\\r\\n        if verbose:\\r\\n            print(f\" decoded {path}  {tuple(wav.shape)} @ {sr} Hz\")\\r\\n        return wav, sr\\r\\n\\r\\n    # ------------------------------------------------------------------ #\\r\\n    def build_dataloaders(\\r\\n        self,\\r\\n        *,\\r\\n        batch_size: int = 8,\\r\\n        val_split: float = 0.15,\\r\\n        test_split: float = 0.15,\\r\\n        target_sr: int = 16_000,\\r\\n        mono: bool = True,\\r\\n        shuffle: bool = True,\\r\\n        num_workers: int = 2,\\r\\n    ):\\r\\n        \"\"\"\\r\\n        Convenience splitter.  Returns:\\r\\n\\r\\n            train_loader, val_loader, test_loader\\r\\n        \"\"\"\\r\\n        ids = self.static_annotations[\"song_id\"].tolist()\\r\\n\\r\\n        # stratify by binary valence (\\xe2\\x89\\xa55) so splits look similar\\r\\n        labels = (self.static_annotations[\"valence_mean\"] >= 5).astype(int).tolist()\\r\\n\\r\\n        train_ids, tmp_ids, train_lbl, tmp_lbl = train_test_split(\\r\\n            ids, labels, test_size=val_split + test_split,\\r\\n            stratify=labels, random_state=42\\r\\n        )\\r\\n        rel_val = val_split / (val_split + test_split)\\r\\n        val_ids, test_ids = train_test_split(\\r\\n            tmp_ids, test_size=1 - rel_val,\\r\\n            stratify=tmp_lbl, random_state=42\\r\\n        )\\r\\n\\r\\n        def _mk(ids_, shuf):\\r\\n            ds = WaveDataset(self, ids_, target_sr=target_sr, mono=mono)\\r\\n            return DataLoader(ds, batch_size=batch_size, shuffle=shuf,\\r\\n                              num_workers=num_workers, pin_memory=True,\\r\\n                              drop_last=True)\\r\\n\\r\\n        return _mk(train_ids, shuffle), _mk(val_ids, False), _mk(test_ids, False)\\r\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ╔══════════════════  REBUILD REAL-SONG CACHE  ═════════════════════╗\n",
        "#   Downloads DEAM (data_utils_old) and saves *only* present IDs\n",
        "# ╚══════════════════════════════════════════════════════════════════╝\n",
        "import os, shutil, glob, torch, torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from data_utils_old import DEAMHandler      # Kaggle-downloading helper\n",
        "\n",
        "# ── Paths ───────────────────────────────────────────────────────────\n",
        "ROOT  = Path(\"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav\")\n",
        "SONGS = ROOT / \"songs\"\n",
        "SONGS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ── 1) Ensure annotations + fresh download ─────────────────────────\n",
        "old_handler = DEAMHandler()                                   # downloads if needed\n",
        "csv_src = old_handler.path / Path(old_handler._STATIC_CSV)\n",
        "shutil.copy(csv_src, ROOT / \"annotations.csv\")\n",
        "print(\"✔ annotations.csv copied\")\n",
        "\n",
        "# ── 2) Purge ALL existing tensors ─────────────────────────────────\n",
        "removed = 0\n",
        "for pt in glob.glob(str(SONGS / \"*.pt\")):\n",
        "    os.remove(pt); removed += 1\n",
        "print(f\"🗑️  removed {removed} old .pt files\")\n",
        "\n",
        "# ── 3) Determine target SR + MAX_LEN (after resample) ─────────────\n",
        "TARGET_SR   = 16_000\n",
        "present_ids = sorted(old_handler.static_annotations[\"song_id\"].tolist())\n",
        "\n",
        "lengths = []\n",
        "print(f\"🔍 Scanning {len(present_ids)} songs @ {TARGET_SR} Hz …\")\n",
        "for sid in tqdm(present_ids, desc=\"probe\"):\n",
        "    wav, _ = old_handler.get_waveform(sid, target_sr=TARGET_SR, mono=True)\n",
        "    lengths.append(wav.shape[-1])\n",
        "\n",
        "MAX_LEN = max(lengths)\n",
        "PAD     = len(str(max(present_ids)))            # zero-pad width\n",
        "print(f\"✔ MAX_LEN = {MAX_LEN} samples ≈ {MAX_LEN/TARGET_SR:.2f} s\\n\")\n",
        "\n",
        "# ── 4) Cache ONLY real songs ───────────────────────────────────────\n",
        "print(f\"💾 Writing {len(present_ids)} tensors into {SONGS}/ …\")\n",
        "for sid in tqdm(present_ids, desc=\"cache\"):\n",
        "    fname   = SONGS / f\"{sid:0{PAD}d}.pt\"\n",
        "    wav_np, _ = old_handler.get_waveform(sid, target_sr=TARGET_SR, mono=True)\n",
        "\n",
        "    wav = torch.as_tensor(wav_np, dtype=torch.float32)[:MAX_LEN]  # 1-D\n",
        "    if wav.numel() < MAX_LEN:                                     # pad end\n",
        "        wav = F.pad(wav, (0, MAX_LEN - wav.numel()))\n",
        "\n",
        "    torch.save(wav.unsqueeze(0), str(fname))                      # [1, MAX_LEN]\n",
        "\n",
        "print(f\"\\n✅ Rebuild complete: {len(present_ids)} real-song tensors saved.\")\n",
        "# ═══════════════════════════════════════════════════════════════════\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903,
          "referenced_widgets": [
            "71ee7991d2c54e62a450af8e8c3b1c24",
            "6977e1598a5c4807b14ccf73d214c83d",
            "0484e61a6ce7459981ede4bfcb883e0c",
            "798486f15ce64eeeabdb5ce728c0d3ce",
            "5ff78c26f1ba40f287a52427fcbb932f",
            "c84310bdacf64134993818314770af50",
            "70176f1e1e1f43c7985b750d78ad9cfc",
            "362c33f186d14d818b4402f3426ee8a7",
            "5f94222fe6b647f5b9078534df29e5cd",
            "3e531302a5d440429e920fd099cd95a7",
            "c09731f906bd4c8bbbd95f33b9897a40",
            "ec6c9ce6d0614ab88a795419c7b9bb68",
            "e16496017952475290022179532f143b",
            "bf0c7af448f34d6089c34a7d2d4b9b46",
            "57a5386ee8df430c9c9316d2f59be9e3",
            "67b091637a5e43d99313df0c03581e02",
            "fb79b6db653b45048b68f019541a546f",
            "fd091359ea3a42fda33e92f6ef40724d",
            "237801d5492841b19edc3478f5e1f689",
            "fd0f75942136450bacaee94876a8a58d",
            "db30f185b5ed45fc81143d6b9be55d70",
            "bb8155c272cf415bafe45f410609368b"
          ]
        },
        "id": "Vl97aki28hg5",
        "outputId": "81c33157-5bc8-465d-aae7-b1961252aea2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading DEAM via kagglehub …\n",
            "Dataset root: /kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music\n",
            "Audio directory: /kaggle/input/deam-mediaeval-dataset-emotional-analysis-in-music/DEAM_audio/MEMD_audio\n",
            "\n",
            "Dataset Preview:\n",
            "   song_id  valence_mean  valence_std  arousal_mean  arousal_std\n",
            "0        2           3.1         0.94           3.0         0.63\n",
            "1        3           3.5         1.75           3.3         1.62\n",
            "2        4           5.7         1.42           5.5         1.63\n",
            "3        5           4.4         2.01           5.3         1.85\n",
            "4        7           5.8         1.47           6.4         1.69\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1744 entries, 0 to 1743\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   song_id       1744 non-null   int64  \n",
            " 1   valence_mean  1744 non-null   float64\n",
            " 2   valence_std   1744 non-null   float64\n",
            " 3   arousal_mean  1744 non-null   float64\n",
            " 4   arousal_std   1744 non-null   float64\n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 68.3 KB\n",
            "None\n",
            "\n",
            "Summary Statistics:\n",
            "           song_id  valence_mean  valence_std  arousal_mean  arousal_std\n",
            "count  1744.000000   1744.000000  1744.000000   1744.000000  1744.000000\n",
            "mean   1062.077982      4.903492     1.521950      4.812603     1.466778\n",
            "std     584.299005      1.173920     0.385496      1.289368     0.367197\n",
            "min       2.000000      1.600000     0.300000      1.600000     0.460000\n",
            "25%     536.750000      4.100000     1.270000      3.800000     1.200000\n",
            "50%    1128.500000      4.900000     1.510000      4.900000     1.480000\n",
            "75%    1564.250000      5.800000     1.780000      5.800000     1.720000\n",
            "max    2000.000000      8.400000     2.900000      8.100000     2.590000\n",
            "✔ annotations.csv copied\n",
            "🗑️  removed 2000 old .pt files\n",
            "🔍 Scanning 1744 songs @ 16000 Hz …\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "probe:   0%|          | 0/1744 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71ee7991d2c54e62a450af8e8c3b1c24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ MAX_LEN = 726579 samples ≈ 45.41 s\n",
            "\n",
            "💾 Writing 1744 tensors into /content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav/songs/ …\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "cache:   0%|          | 0/1744 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec6c9ce6d0614ab88a795419c7b9bb68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Rebuild complete: 1744 real-song tensors saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Remove leading zeros from .pt filenames in DEAM_wav/songs/\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Adjust this path to your project location\n",
        "SONGS_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav/songs\")\n",
        "\n",
        "renamed = 0\n",
        "skipped = 0\n",
        "\n",
        "for pt_file in SONGS_DIR.glob(\"*.pt\"):\n",
        "    stem = pt_file.stem  # filename without extension\n",
        "    # Only process numeric stems that start with '0'\n",
        "    if stem.isdigit() and stem.startswith(\"0\"):\n",
        "        raw_id = str(int(stem))           # \"0079\" → \"79\"\n",
        "        new_name = f\"{raw_id}.pt\"\n",
        "        new_path = SONGS_DIR / new_name\n",
        "\n",
        "        if not new_path.exists():\n",
        "            os.rename(pt_file, new_path)\n",
        "            renamed += 1\n",
        "        else:\n",
        "            skipped += 1\n",
        "    else:\n",
        "        skipped += 1\n",
        "\n",
        "print(f\"✅ Renamed {renamed} files (removed padding).\")\n",
        "print(f\"⏭️  Skipped {skipped} files (already raw-ID or conflict).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtlVpidIBZiS",
        "outputId": "7dc6a286-5295-4aad-e56a-43d96ebb485b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Renamed 743 files (removed padding).\n",
            "⏭️  Skipped 1001 files (already raw-ID or conflict).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further  pre-process\n"
      ],
      "metadata": {
        "id": "AI19DNsKYlFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading new data_utils"
      ],
      "metadata": {
        "id": "fOhX3JoAZEG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RvCof407ZILr",
        "outputId": "61df4566-33e5-4ef2-8c58-4b5fa6263f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-78526a65-ea7c-43b9-ad28-1b17648c5e76\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-78526a65-ea7c-43b9-ad28-1b17648c5e76\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving data_utils.py to data_utils.py\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_utils.py': b'# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80 data_utils.py \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\nfrom __future__ import annotations\\r\\n\\r\\nimport os\\r\\nimport torch\\r\\nimport torchaudio\\r\\nimport pandas as pd\\r\\nfrom sklearn.model_selection import train_test_split\\r\\nfrom torch.utils.data import Dataset, DataLoader\\r\\n\\r\\n__all__ = [\"DEAMHandler\", \"WaveDataset\"]\\r\\n\\r\\n\\r\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80  WaveDataset  \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\nclass WaveDataset(Dataset):\\r\\n    \"\"\"\\r\\n    Yields (wave_tensor, label_tensor):\\r\\n        wave  : torch.FloatTensor, shape = [1, T]\\r\\n        label : torch.FloatTensor, shape = [2]   (valence_mean, arousal_mean)\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, handler: \"DEAMHandler\", ids, target_sr=16_000, mono=True):\\r\\n        self.h, self.ids = handler, ids\\r\\n        self.target_sr, self.mono = target_sr, mono\\r\\n\\r\\n        # cache labels in a dict: sid -> (valence, arousal)\\r\\n        df = handler.static_annotations.set_index(\"song_id\")\\r\\n        self._labels = {\\r\\n            sid: (row.valence_mean, row.arousal_mean)\\r\\n            for sid, row in df.iterrows()\\r\\n        }\\r\\n\\r\\n    def __len__(self):  return len(self.ids)\\r\\n\\r\\n    def __getitem__(self, idx):\\r\\n        sid        = self.ids[idx]\\r\\n        wav, _     = self.h.get_waveform(sid,\\r\\n                                         target_sr=self.target_sr,\\r\\n                                         mono=self.mono)\\r\\n        val, aro   = self._labels[sid]\\r\\n        label      = torch.tensor([val, aro], dtype=torch.float32)\\r\\n        return wav, label\\r\\n\\r\\n\\r\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80  DEAMHandler  \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\nclass DEAMHandler:\\r\\n    \"\"\"\\r\\n    Lightweight wrapper around *your own* DEAM copy.\\r\\n\\r\\n    Parameters\\r\\n    ----------\\r\\n    audio_root : str\\r\\n        Folder that contains files named  <song_id>.wav   (or <song_id>.pt).\\r\\n    annotations_csv : str | None, default = None\\r\\n        Path to static_annotations.csv.  If None, we look for it:\\r\\n            \\xe2\\x80\\xa2 audio_root/static_annotations.csv\\r\\n            \\xe2\\x80\\xa2 parent_of_audio_root/static_annotations.csv\\r\\n    \"\"\"\\r\\n\\r\\n    def __init__(self, *, audio_root: str, annotations_csv: str | None = None):\\r\\n        # -------- verify / locate paths -----------------------------------\\r\\n        if not os.path.isdir(audio_root):\\r\\n            raise FileNotFoundError(f\"audio_root not found: {audio_root!r}\")\\r\\n        self.audio_dir = os.path.join(audio_root, \"songs\")\\r\\n        if not os.path.isdir(self.audio_dir):\\r\\n            raise FileNotFoundError(f\"songs/ directory not found in {audio_root}\")\\r\\n\\r\\n        if annotations_csv is None:\\r\\n            candidates = [\\r\\n                os.path.join(audio_root, \"static_annotations.csv\"),\\r\\n                os.path.join(os.path.dirname(audio_root), \"static_annotations.csv\"),\\r\\n            ]\\r\\n            annotations_csv = next((p for p in candidates if os.path.isfile(p)), None)\\r\\n        if annotations_csv is None or not os.path.isfile(annotations_csv):\\r\\n            raise FileNotFoundError(\\r\\n                \"static_annotations.csv not found.\\\\n\"\\r\\n                \"Place it inside your audio folder or pass annotations_csv=\\'\\xe2\\x80\\xa6\\'\"\\r\\n            )\\r\\n\\r\\n        # -------- load annotations ---------------------------------------\\r\\n        self.static_annotations = pd.read_csv(annotations_csv)\\r\\n        self.static_annotations.columns = self.static_annotations.columns.str.strip()  # NEW\\r\\n        # remove stray leading/trailing spaces sometimes found in the CSV header\\r\\n        self.static_annotations.columns = (\\r\\n            self.static_annotations.columns.str.strip()\\r\\n        )\\r\\n        print(\"\\xe2\\x9c\\x94 static annotations :\", annotations_csv)\\r\\n        print(\"\\xe2\\x9c\\x94 audio root         :\", self.audio_dir)\\r\\n\\r\\n    # ------------------------------------------------------------------ #\\r\\n    def get_waveform(\\r\\n        self,\\r\\n        song_id: int,\\r\\n        *,\\r\\n        target_sr: int | None = 16_000,\\r\\n        mono: bool = True,\\r\\n        verbose: bool = False,\\r\\n    ):\\r\\n        \"\"\"\\r\\n        Return (wave_tensor, sr).  Search order in audio_root:\\r\\n\\r\\n            1. <song_id>.pt   (pre-saved torch tensor)    \\xe2\\x80\\x93 fastest\\r\\n            2. <song_id>.wav  (PCM, no decode penalty)\\r\\n\\r\\n        Raises FileNotFoundError if neither exists.\\r\\n        \"\"\"\\r\\n        for ext in (\".pt\", \".wav\"):\\r\\n            path = os.path.join(self.audio_dir, f\"{song_id}{ext}\")\\r\\n            if os.path.isfile(path):\\r\\n                break\\r\\n        else:\\r\\n            raise FileNotFoundError(f\"no .pt or .wav for song_id {song_id}\")\\r\\n\\r\\n        # ----- cached tensor --------------------------------------------\\r\\n        if ext == \".pt\":\\r\\n            wav = torch.load(path, map_location=\"cpu\").float()\\r\\n            # legacy caches may be 1-D \\xe2\\x86\\x92 add channel dim\\r\\n            if wav.dim() == 1:\\r\\n                print(f\"Warning: {path} is 1-D, adding channel dim - should not happen\")\\r\\n                wav = wav.unsqueeze(0)\\r\\n            sr  = target_sr or 16_000\\r\\n            if verbose:\\r\\n                print(f\" loaded tensor {path}  {tuple(wav.shape)}\")\\r\\n            return wav, sr\\r\\n\\r\\n        # ----- WAV via torchaudio ---------------------------------------\\r\\n        wav, sr = torchaudio.load(path)          # [ch, T]\\r\\n        if mono and wav.shape[0] > 1:\\r\\n            wav = wav.mean(dim=0, keepdim=True)\\r\\n        if target_sr and sr != target_sr:\\r\\n            wav = torchaudio.functional.resample(wav, sr, target_sr)\\r\\n            sr  = target_sr\\r\\n        if verbose:\\r\\n            print(f\" decoded {path}  {tuple(wav.shape)} @ {sr} Hz\")\\r\\n        return wav, sr\\r\\n\\r\\n    # ------------------------------------------------------------------ #\\r\\n    def build_dataloaders(\\r\\n        self,\\r\\n        *,\\r\\n        batch_size: int = 8,\\r\\n        val_split: float = 0.15,\\r\\n        test_split: float = 0.15,\\r\\n        target_sr: int = 16_000,\\r\\n        mono: bool = True,\\r\\n        shuffle: bool = True,\\r\\n        num_workers: int = 2,\\r\\n    ):\\r\\n        \"\"\"\\r\\n        Convenience splitter.  Returns:\\r\\n\\r\\n            train_loader, val_loader, test_loader\\r\\n        \"\"\"\\r\\n        ids = self.static_annotations[\"song_id\"].tolist()\\r\\n\\r\\n        ids = [sid for sid in ids\\r\\n           if os.path.isfile(os.path.join(self.audio_dir, f\"{sid}.pt\"))]\\r\\n\\r\\n        # stratify by binary valence (\\xe2\\x89\\xa55) so splits look similar\\r\\n        labels = (self.static_annotations[\"valence_mean\"] >= 5).astype(int).tolist()\\r\\n\\r\\n        train_ids, tmp_ids, train_lbl, tmp_lbl = train_test_split(\\r\\n            ids, labels, test_size=val_split + test_split,\\r\\n            stratify=labels, random_state=42\\r\\n        )\\r\\n        rel_val = val_split / (val_split + test_split)\\r\\n        val_ids, test_ids = train_test_split(\\r\\n            tmp_ids, test_size=1 - rel_val,\\r\\n            stratify=tmp_lbl, random_state=42\\r\\n        )\\r\\n\\r\\n        def _mk(ids_, shuf):\\r\\n            ds = WaveDataset(self, ids_, target_sr=target_sr, mono=mono)\\r\\n            return DataLoader(ds, batch_size=batch_size, shuffle=shuf,\\r\\n                              num_workers=num_workers, pin_memory=True,\\r\\n                              drop_last=True)\\r\\n\\r\\n        return _mk(train_ids, shuffle), _mk(val_ids, False), _mk(test_ids, False)\\r\\n# \\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\xe2\\x94\\x80\\r\\n'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4FwCNVwhPO2",
        "outputId": "0664f282-cb4f-4817-dc7f-79e54a7ce442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# precompute_wav2vec_feats.py  (fixed)\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "import os, torch\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoFeatureExtractor, Wav2Vec2Model\n",
        "from data_utils import DEAMHandler\n",
        "\n",
        "AUDIO_ROOT = \"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav\"\n",
        "MODEL_ID   = \"ALM/wav2vec2-base-audioset\"\n",
        "OUT_DIR    = \"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_feats/songs\"\n",
        "BATCH      = 16                  # adjust to your GPU memory\n",
        "DTYPE      = torch.float16       # or torch.float32\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "encoder  = (\n",
        "    Wav2Vec2Model\n",
        "    .from_pretrained(MODEL_ID, output_hidden_states=True)\n",
        "    .eval()\n",
        "    .to(device)\n",
        ")\n",
        "feature_extractor = AutoFeatureExtractor.from_pretrained(MODEL_ID)\n",
        "for p in encoder.parameters():\n",
        "    p.requires_grad_(False)\n",
        "\n",
        "handler  = DEAMHandler(audio_root=AUDIO_ROOT)\n",
        "song_ids = handler.static_annotations[\"song_id\"].tolist()\n",
        "\n",
        "def slices(lst, n):\n",
        "    \"\"\"Yield successive chunks of size n.\"\"\"\n",
        "    for i in range(0, len(lst), n):\n",
        "        yield lst[i : i + n]\n",
        "\n",
        "for chunk in tqdm(list(slices(song_ids, BATCH)), desc=\"songs\"):\n",
        "    waves, done_ids = [], []\n",
        "    for sid in chunk:\n",
        "        out_path = os.path.join(OUT_DIR, f\"{sid}.pt\")\n",
        "        if os.path.exists(out_path):\n",
        "            continue\n",
        "        wav, _ = handler.get_waveform(sid, target_sr=16_000, mono=True)\n",
        "        waves.append(wav.squeeze().numpy())\n",
        "        done_ids.append((sid, out_path))\n",
        "    if not waves:\n",
        "        continue\n",
        "\n",
        "    inputs = feature_extractor(\n",
        "        waves,\n",
        "        sampling_rate=16_000,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,              # pad to longest in batch\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # hidden_states is a tuple: [layer0, layer1, ..., layerN]\n",
        "        # we want the output just *before* the last 2 layers:\n",
        "        h_states = encoder(\n",
        "            inputs.input_values.to(device)\n",
        "        ).hidden_states[-3]         # shape [B, T_max, 768]\n",
        "\n",
        "    frame_counts = inputs.attention_mask.sum(1)  # valid lengths per example\n",
        "\n",
        "    # ── FIXED: iterate by batch index, slice time axis ──\n",
        "    for i, ((sid, out_path), n) in enumerate(zip(done_ids, frame_counts)):\n",
        "        # take the i-th example in the batch, then slice off padding\n",
        "        single = h_states[i, : n].to(\"cpu\", dtype=DTYPE).contiguous()  # [T_i, 768]\n",
        "        torch.save(single, out_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KyMeAVWgb9C",
        "outputId": "b07386ac-f4fd-47e8-ef2b-a9623c0b4fbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ static annotations : /content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav/static_annotations.csv\n",
            "✔ audio root         : /content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav/songs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "songs: 100%|██████████| 109/109 [1:00:40<00:00, 33.40s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Arranging the data's folder"
      ],
      "metadata": {
        "id": "D19XUsY1zVt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "FEAT_ROOT = Path(\"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_feats\")\n",
        "WAV_ROOT  = Path(\"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav\")\n",
        "\n",
        "SONG_DST  = FEAT_ROOT / \"songs\"\n",
        "SONG_DST.mkdir(exist_ok=True)\n",
        "\n",
        "#  move every *.pt\n",
        "for pt_file in FEAT_ROOT.glob(\"*.pt\"):\n",
        "    shutil.move(pt_file, SONG_DST / pt_file.name)\n",
        "\n",
        "# move the annotations CSV\n",
        "ann_src = WAV_ROOT / \"static_annotations.csv\"\n",
        "ann_dst = FEAT_ROOT / ann_src.name\n",
        "if ann_src.exists():\n",
        "    shutil.move(ann_src, ann_dst)\n",
        "    print(f\"moved {ann_src} → {ann_dst}\")\n",
        "else:\n",
        "    print(\"⚠️  static_annotations.csv not found in DEAM_wav\")\n",
        "\n",
        "# confirm counts\n",
        "print(f\"{len(list((SONG_DST).glob('*.pt')))} feature files now in {SONG_DST}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqnBegNWha9Y",
        "outputId": "c0e7e9e9-6914-40e0-e0f5-cbe72f111e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "moved /content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_wav/static_annotations.csv → /content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_feats/static_annotations.csv\n",
            "1744 feature files now in /content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_feats/songs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### quick check - can remove"
      ],
      "metadata": {
        "id": "xOhZpWzQ1rXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path, PurePosixPath\n",
        "import shutil, itertools\n",
        "\n",
        "FEAT_ROOT = Path(\"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_feats\")\n",
        "SONG_DST  = FEAT_ROOT / \"songs\"\n",
        "\n",
        "parent_pts = list(FEAT_ROOT.glob(\"*.pt\"))\n",
        "child_pts  = list(SONG_DST.glob(\"*.pt\"))\n",
        "\n",
        "print(f\"In parent: {len(parent_pts)}   in songs/: {len(child_pts)}\")\n",
        "\n",
        "# ── move any stragglers ───────────────────────────────────────────────\n",
        "for f in parent_pts:\n",
        "    target = SONG_DST / f.name\n",
        "    if not target.exists():\n",
        "        shutil.move(f, target)\n",
        "        print(\"moved\", f.name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o134yTlRzvJ8",
        "outputId": "7a5cf716-4ad8-45f7-c598-75d5bfa0baf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In parent: 0   in songs/: 1744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "list(glob.glob(\"/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_feats/**/static_annotations.csv\",\n",
        "               recursive=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2ftqKGa1uav",
        "outputId": "22e2f742-13c8-4515-c5b4-caa8c72e07a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/Colab Notebooks/DeepProject/DEAM_feats/static_annotations.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W_Aqvj1l19I5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}